# taylordgenes-variant-calling: Benchmarking

## Overview

This pipeline provides an optional benchmarking mode to evaluate germline small variant calling performance against a trusted truth set. Benchmarking is implemented as a dedicated subworkflow and is intended for pipeline validation and regression testing.

Benchmarking is disabled by default and must be explicitly enabled via a dedicated Nextflow profile.

## Benchmark sample

### Sample selection

Benchmarking is performed using HG002 (NA24385) from the Genome in a Bottle (GIAB) Ashkenazim Trio.
HG002 is used because it is the current community standard for germline variant benchmarking and is supported by:
* High-confidence truth variant sets curated by NIST
* Well-defined benchmark regions
* Extensive use in published benchmarking studies and challenges

This ensures results are comparable to published benchmarks and other pipelines.

### Reference genome
* Genome build: GRCh38
* GIAB truth set version: v4.2.1

The following truth files are used:
* High-confidence truth VCF (SNVs and indels)
* High-confidence benchmark BED regions

Only variants falling within the benchmark regions are evaluated.

## Input data and FASTQ selection

### Available sequencing data

HG002 Illumina whole-genome sequencing data is available as multiple FASTQ pairs across many sequencing lanes, representing very high total coverage.

Running all available reads is computationally expensive and was unnecessary for this exercise.

### Resource-aware execution

For this pipeline:
* A single FASTQ pair (one sequencing lane) is used
* All reads within that FASTQ pair are processed
* No read downsampling is performed within the file

This approach was chosen to reduce runtime and storage requirements, enable rapid development and testing while still allow meaningful comparison against the truth set.

This benchmarking mode is intended to validate correctness rather than to estimate absolute sensitivity at maximum coverage.

## Benchmarking scope

Benchmarking evaluates small variants only such as Single-nucleotide variants (SNVs) and Small insertions and deletions (indels).

Excluded variant types are Structural variants (SVs) and Copy-number variants (CNVs). These variant classes require different benchmarking strategies and truth sets and are outside the scope of this benchmarking subworkflow.

## Benchmarking workflow

Query VCFs generated by the pipeline are normalized prior to benchmarking to ensure consistent representation.
Normalization includes, left-alignment of indels splitting multiallelic records and reference-consistent representation.

This step is required because benchmarking tools expect canonical variant representations. The tool used is bcftools norm.

## Comparison to truth set
Normalized query variants are compared to the GIAB truth set using hap.py, a widely adopted benchmarking tool for germline small variants.
hap.py:
* Restricts evaluation to high-confidence regions
* Handles complex variant representations
* Reports precision, recall, and F1-score metrics
* Produces stratified and per-variant comparison outputs

## Outputs
The benchmarking subworkflow produces the following outputs:
* Summary CSV files with precision, recall, and F1 scores
* Extended comparison tables
* Annotated VCFs labeling true positives, false positives, and false negatives
* Log files for traceability

## Enabling benchmarking

Use the benchmarking profile:
```{bash}
nextflow run main.nf \
  -profile benchmarking,docker \
  --input benchmarking_samplesheet.csv \
  --outdir results/
```